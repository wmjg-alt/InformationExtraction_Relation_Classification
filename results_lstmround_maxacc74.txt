
D:\SPRING2023\Information Extraction\ASSIGNMENT3_IE\recitation9_code\solution>python main.py --batch-size 16 --max-epochs 32 --device cuda --train-file ../data/semevalTrainReal.tsv --dev-file ../data/semevalDev.tsv --lr 0.00005 --debug --truncate & python main.py --batch-size 16 --max-epochs 16 --device cuda --train-file ../data/semevalTrainReal.tsv --dev-file ../data/semevalDev.tsv --lr 0.00005 --debug --llm --truncate --llm-choice distilbert-base-uncased & python main.py --batch-size 16 --max-epochs 16 --device cuda --train-file ../data/semevalTrainReal.tsv --dev-file ../data/semevalDev.tsv --lr 0.00005 --debug --llm --truncate --llm-choice bert-base-uncased & python main.py --batch-size 16 --max-epochs 16 --device cuda --train-file ../data/semevalTrainReal.tsv --dev-file ../data/semevalDev.tsv --lr 0.00005 --debug --llm --truncate --llm-choice openai-gpt & python main.py --batch-size 16 --max-epochs 32 --device cuda --train-file ../data/semevalTrainReal.tsv --dev-file ../data/semevalDev.tsv --lr 0.00005 --debug & python main.py --batch-size 16 --max-epochs 16 --device cuda --train-file ../data/semevalTrainReal.tsv --dev-file ../data/semevalDev.tsv --lr 0.00005 --debug --llm  --llm-choice distilbert-base-uncased & python main.py --batch-size 16 --max-epochs 16 --device cuda --train-file ../data/semevalTrainReal.tsv --dev-file ../data/semevalDev.tsv --lr 0.00005 --debug --llm  --llm-choice bert-base-uncased & python main.py --batch-size 16 --max-epochs 16 --device cuda --train-file ../data/semevalTrainReal.tsv --dev-file ../data/semevalDev.tsv --lr 0.00005 --debug --llm  --llm-choice openai-gpt
EXPERIMENT False None True 5e-05
  epoch    f1_macro    f1_micro    train_loss    valid_acc    valid_loss     dur
-------  ----------  ----------  ------------  -----------  ------------  ------
      1      0.0167      0.1775        2.7574       0.1775        2.7005  3.0480
      2      0.0167      0.1775        2.6851       0.1775        2.6920  2.6270
      3      0.0167      0.1775        2.6789       0.1775        2.6886  2.9014
      4      0.0167      0.1775        2.6754       0.1775        2.6862  3.0089
      5      0.0167      0.1775        2.6722       0.1775        2.6847  2.7571
      6      0.0167      0.1775        2.6687       0.1775        2.6832  2.5341
      7      0.0167      0.1775        2.6639       0.1775        2.6811  2.5126
      8      0.0167      0.1775        2.6565       0.1775        2.6773  2.5182
      9      0.0167      0.1775        2.6431       0.1775        2.6687  2.5769
     10      0.0167      0.1775        2.6190       0.1775        2.6475  2.5120
     11      0.0341      0.1885        2.5759       0.1885        2.6102  2.5610
     12      0.0615      0.2185        2.5149       0.2185        2.5595  2.5556
     13      0.0803      0.2405        2.4351       0.2405        2.4962  2.5210
     14      0.1087      0.2775        2.3443       0.2775        2.4292  2.5306
     15      0.1314      0.3000        2.2488       0.3000        2.3655  2.5400
     16      0.1473      0.3160        2.1515       0.3160        2.3023  2.5570
     17      0.1624      0.3300        2.0545       0.3300        2.2474  2.5470
     18      0.1693      0.3405        1.9613       0.3405        2.1965  2.5835
     19      0.1781      0.3480        1.8731       0.3480        2.1559  2.5580
     20      0.1809      0.3505        1.7877       0.3505        2.1213  2.5660
     21      0.1879      0.3525        1.7046       0.3525        2.0989  2.6063
     22      0.2006      0.3610        1.6276       0.3610        2.0716  2.5330
     23      0.2114      0.3635        1.5525       0.3635        2.0518  2.5640
     24      0.2216      0.3665        1.4780       0.3665        2.0338  2.5510
     25      0.2259      0.3665        1.4091       0.3665        2.0297  2.5418
     26      0.2303      0.3600        1.3431       0.3600        2.0426  2.5422
     27      0.2331      0.3570        1.2794       0.3570        2.0410  2.5398
     28      0.2427      0.3610        1.2201       0.3610        2.0301  2.5401
     29      0.2475      0.3550        1.1612       0.3550        2.0422  2.5380
     30      0.2578      0.3590        1.1053       0.3590        2.0452  2.5330
     31      0.2630      0.3640        1.0528       0.3640        2.0198  2.5414
     32      0.2696      0.3725        1.0020       0.3725        1.9945  2.5370
NOTE: Redirects are currently not supported in Windows or MacOs.
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
EXPERIMENT True distilbert-base-uncased True 5e-05
  epoch    f1_macro    f1_micro    train_loss    valid_acc    valid_loss      dur
-------  ----------  ----------  ------------  -----------  ------------  -------
      1      0.3367      0.4720        2.3865       0.4720        1.8860  18.5910
      2      0.5287      0.5800        1.4453       0.5800        1.4837  18.3654
      3      0.5737      0.6100        0.9892       0.6100        1.4125  18.1756
      4      0.6001      0.6210        0.6764       0.6210        1.4816  17.9731
      5      0.6026      0.6180        0.4637       0.6180        1.6110  18.1822
      6      0.6004      0.6080        0.3431       0.6080        1.7170  18.1074
      7      0.6194      0.6190        0.2636       0.6190        1.8401  18.1400
      8      0.6025      0.5990        0.2015       0.5990        1.9440  18.0206
      9      0.6097      0.6090        0.1824       0.6090        2.0117  18.2634
     10      0.6093      0.6085        0.1575       0.6085        2.0314  17.8443
     11      0.6153      0.6105        0.1303       0.6105        2.0654  17.9437
     12      0.6021      0.6070        0.1146       0.6070        2.1782  17.9145
     13      0.6049      0.5990        0.0983       0.5990        2.2789  18.0705
     14      0.6135      0.6020        0.0931       0.6020        2.2970  17.8778
     15      0.6166      0.6105        0.0989       0.6105        2.2860  17.9558
     16      0.6089      0.6120        0.0799       0.6120        2.2790  17.8668
NOTE: Redirects are currently not supported in Windows or MacOs.
Downloading: 100%|███████████████████████████████████████| 28.0/28.0 [00:00<00:00, 7.09kB/s]
C:\Users\William\AppData\Local\Programs\Python\Python310\lib\site-packages\huggingface_hub\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\William\.cache\huggingface\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
Downloading: 100%|██████████████████████████████████████████| 570/570 [00:00<00:00, 143kB/s]
Downloading: 100%|███████████████████████████████████████| 232k/232k [00:00<00:00, 7.81MB/s]
Downloading: 100%|███████████████████████████████████████| 466k/466k [00:00<00:00, 8.51MB/s]
Downloading: 100%|███████████████████████████████████████| 440M/440M [00:05<00:00, 79.5MB/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
EXPERIMENT True bert-base-uncased True 5e-05
  epoch    f1_macro    f1_micro    train_loss    valid_acc    valid_loss      dur
-------  ----------  ----------  ------------  -----------  ------------  -------
      1      0.2386      0.3935        2.5198       0.3935        2.1136  27.5274
      2      0.3849      0.5060        1.7572       0.5060        1.7020  26.9483
      3      0.4420      0.5245        1.3527       0.5245        1.6800  26.9336
      4      0.5381      0.5870        1.0902       0.5870        1.5620  27.4390
      5      0.5690      0.6095        0.8781       0.6095        1.5555  26.9309
      6      0.5927      0.6090        0.6993       0.6090        1.6270  27.0200
      7      0.6012      0.6210        0.5626       0.6210        1.6806  27.1652
      8      0.5930      0.6055        0.4988       0.6055        1.7310  26.9583
      9      0.5917      0.6010        0.4093       0.6010        1.7770  26.8312
     10      0.6211      0.6200        0.3265       0.6200        1.8221  27.1301
     11      0.6105      0.6145        0.2925       0.6145        1.9217  26.8910
     12      0.6155      0.6170        0.2685       0.6170        1.9181  26.9900
     13      0.6007      0.6110        0.2512       0.6110        1.9134  26.8764
     14      0.6079      0.6135        0.2229       0.6135        1.9876  26.7980
     15      0.6136      0.6135        0.2150       0.6135        2.0715  26.9140
     16      0.6151      0.6160        0.1974       0.6160        2.0534  27.1950
NOTE: Redirects are currently not supported in Windows or MacOs.
EXPERIMENT True openai-gpt True 5e-05
  epoch    f1_macro    f1_micro    train_loss    valid_acc    valid_loss      dur
-------  ----------  ----------  ------------  -----------  ------------  -------
      1      0.4653      0.5580        2.3114       0.5580        1.6359  25.5198
      2      0.5537      0.6050        1.3884       0.6050        1.3809  25.1625
      3      0.5997      0.6330        0.9657       0.6330        1.3641  25.2730
      4      0.6463      0.6520        0.6810       0.6520        1.3243  25.0223
      5      0.6540      0.6540        0.4692       0.6540        1.3927  25.0352
      6      0.6617      0.6570        0.3281       0.6570        1.4627  25.2503
      7      0.6666      0.6605        0.2487       0.6605        1.5713  25.1644
      8      0.6527      0.6465        0.1952       0.6465        1.6337  25.3261
      9      0.6390      0.6445        0.1433       0.6445        1.7354  25.3259
     10      0.6472      0.6310        0.1237       0.6310        1.9137  25.1742
     11      0.6525      0.6485        0.1157       0.6485        1.8342  24.8792
     12      0.6592      0.6600        0.1070       0.6600        1.8361  24.9619
     13      0.6450      0.6370        0.1186       0.6370        1.9362  25.0817
     14      0.6554      0.6540        0.0896       0.6540        1.8727  24.9468
     15      0.6470      0.6475        0.0865       0.6475        1.9475  25.2083
     16      0.6524      0.6475        0.0821       0.6475        2.0252  25.1181
EXPERIMENT False None False 5e-05
  epoch    f1_macro    f1_micro    train_loss    valid_acc    valid_loss     dur
-------  ----------  ----------  ------------  -----------  ------------  ------
      1      0.0167      0.1775        2.7610       0.1775        2.7006  3.2570
      2      0.0167      0.1775        2.6832       0.1775        2.6920  2.9800
      3      0.0167      0.1775        2.6779       0.1775        2.6890  2.9780
      4      0.0167      0.1775        2.6748       0.1775        2.6873  2.9740
      5      0.0167      0.1775        2.6720       0.1775        2.6860  2.9848
      6      0.0167      0.1775        2.6688       0.1775        2.6847  3.0132
      7      0.0167      0.1775        2.6643       0.1775        2.6824  2.9720
      8      0.0167      0.1775        2.6576       0.1775        2.6781  3.0442
      9      0.0178      0.1785        2.6473       0.1785        2.6705  3.0368
     10      0.0302      0.1880        2.6315       0.1880        2.6586  2.9805
     11      0.0368      0.1950        2.6079       0.1950        2.6408  2.9636
     12      0.0441      0.1950        2.5744       0.1950        2.6191  2.9959
     13      0.0497      0.1995        2.5336       0.1995        2.5998  2.9720
     14      0.0552      0.2090        2.4915       0.2090        2.5826  2.9903
     15      0.0603      0.2180        2.4513       0.2180        2.5692  2.9990
     16      0.0632      0.2220        2.4123       0.2220        2.5600  2.9858
     17      0.0671      0.2285        2.3736       0.2285        2.5547  3.0210
     18      0.0681      0.2245        2.3349       0.2245        2.5505  2.9680
     19      0.0716      0.2230        2.2943       0.2230        2.5488  2.9710
     20      0.0752      0.2225        2.2534       0.2225        2.5480  2.9830
     21      0.0743      0.2160        2.2132       0.2160        2.5508  2.9883
     22      0.0750      0.2155        2.1727       0.2155        2.5542  3.0396
     23      0.0768      0.2145        2.1338       0.2145        2.5532  3.0634
     24      0.0762      0.2115        2.0952       0.2115        2.5634  3.0020
     25      0.0777      0.2100        2.0565       0.2100        2.5771  2.9942
     26      0.0788      0.2090        2.0193       0.2090        2.5765  2.9888
     27      0.0801      0.2060        1.9827       0.2060        2.5854  2.9810
     28      0.0831      0.2070        1.9461       0.2070        2.5812  2.9810
     29      0.0910      0.2100        1.9112       0.2100        2.5970  2.9920
     30      0.0870      0.2030        1.8790       0.2030        2.6185  3.0040
     31      0.0927      0.2040        1.8492       0.2040        2.6149  2.9780
     32      0.0909      0.2025        1.8165       0.2025        2.6314  2.9830
NOTE: Redirects are currently not supported in Windows or MacOs.
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
EXPERIMENT True distilbert-base-uncased False 5e-05
  epoch    f1_macro    f1_micro    train_loss    valid_acc    valid_loss      dur
-------  ----------  ----------  ------------  -----------  ------------  -------
      1      0.2337      0.4115        2.5261       0.4115        2.0475  20.3586
      2      0.5504      0.6160        1.6124       0.6160        1.3962  19.9931
      3      0.6252      0.6685        0.9421       0.6685        1.2162  20.2223
      4      0.6636      0.6780        0.5652       0.6780        1.2579  20.0508
      5      0.7015      0.7005        0.3629       0.7005        1.2501  20.0349
      6      0.7154      0.7110        0.2462       0.7110        1.2610  19.9827
      7      0.7065      0.7040        0.1806       0.7040        1.3583  20.0345
      8      0.6875      0.6980        0.1300       0.6980        1.4309  19.9856
      9      0.7128      0.7085        0.0833       0.7085        1.4708  20.0088
     10      0.7093      0.7150        0.0714       0.7150        1.4697  20.0730
     11      0.7112      0.7020        0.0881       0.7020        1.5927  20.1247
     12      0.7094      0.7055        0.0856       0.7055        1.5434  20.0126
     13      0.7139      0.7095        0.0774       0.7095        1.5500  20.2611
     14      0.6980      0.6970        0.0595       0.6970        1.6551  20.4972
     15      0.7003      0.7070        0.0401       0.7070        1.6717  20.3727
     16      0.7022      0.7030        0.0337       0.7030        1.7380  20.1306
NOTE: Redirects are currently not supported in Windows or MacOs.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
EXPERIMENT True bert-base-uncased False 5e-05
  epoch    f1_macro    f1_micro    train_loss    valid_acc    valid_loss      dur
-------  ----------  ----------  ------------  -----------  ------------  -------
      1      0.2442      0.4430        2.5081       0.4430        1.9300  32.7037
      2      0.6058      0.6665        1.4542       0.6665        1.2195  31.9940
      3      0.6649      0.7115        0.8763       0.7115        1.0664  32.4890
      4      0.6892      0.7085        0.5696       0.7085        1.0927  31.8974
      5      0.7295      0.7315        0.3876       0.7315        1.1257  32.7170
      6      0.7202      0.7235        0.3019       0.7235        1.1648  32.0038
      7      0.7406      0.7430        0.2185       0.7430        1.1529  32.1148
      8      0.7328      0.7210        0.1495       0.7210        1.2891  32.3481
      9      0.7305      0.7400        0.1365       0.7400        1.2725  31.8269
     10      0.7316      0.7315        0.1177       0.7315        1.3318  32.0299
     11      0.7548      0.7495        0.1048       0.7495        1.3312  32.2954
     12      0.6995      0.7080        0.1083       0.7080        1.4940  31.7745
     13      0.7103      0.7260        0.0973       0.7260        1.4285  32.1273
     14      0.7443      0.7430        0.0800       0.7430        1.3743  31.6052
     15      0.7399      0.7360        0.0640       0.7360        1.4781  31.6327
     16      0.7299      0.7310        0.0710       0.7310        1.4609  32.0991
NOTE: Redirects are currently not supported in Windows or MacOs.
EXPERIMENT True openai-gpt False 5e-05
  epoch    f1_macro    f1_micro    train_loss    valid_acc    valid_loss      dur
-------  ----------  ----------  ------------  -----------  ------------  -------
      1      0.3431      0.5010        2.3361       0.5010        1.7485  33.7569
      2      0.5614      0.6285        1.3627       0.6285        1.3461  33.4565
      3      0.6119      0.6700        0.9278       0.6700        1.2173  33.4415
      4      0.6302      0.6660        0.6658       0.6660        1.2913  33.7688
      5      0.6438      0.6690        0.4713       0.6690        1.3659  33.4793
      6      0.6758      0.6830        0.3486       0.6830        1.3376  33.2515
      7      0.6802      0.6910        0.2637       0.6910        1.3431  33.2030
      8      0.6703      0.6860        0.1828       0.6860        1.4552  33.1396
      9      0.6822      0.6780        0.1722       0.6780        1.5453  33.2339
     10      0.6875      0.6860        0.1424       0.6860        1.5638  33.4598
     11      0.6956      0.6950        0.1286       0.6950        1.5363  33.2461
     12      0.6738      0.6725        0.0991       0.6725        1.6024  33.2767
     13      0.6901      0.6820        0.0692       0.6820        1.6586  33.7156
     14      0.6857      0.6860        0.0677       0.6860        1.6863  33.1140
     15      0.6840      0.6885        0.0713       0.6885        1.6542  33.2432
     16      0.6728      0.6750        0.0728       0.6750        1.7641  33.8601

D:\SPRING2023\Information Extraction\ASSIGNMENT3_IE\recitation9_code\solution>